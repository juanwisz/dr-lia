{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install openai\n",
        "!pip install requests\n",
        "!pip install tenacity"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOz4LPo_r10W",
        "outputId": "53d2eb42-d702-4e3e-fd3f-08e70779890c"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (0.27.8)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tenacity in /usr/local/lib/python3.10/dist-packages (8.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import openai\n",
        "import requests\n",
        "from tenacity import retry, wait_random_exponential, stop_after_attempt\n",
        "\n",
        "GPT_MODEL = \"gpt-3.5-turbo-0613\"\n",
        "openai.api_key = ''"
      ],
      "metadata": {
        "id": "aqUPvddZsAMb"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@retry(wait=wait_random_exponential(min=1, max=40), stop=stop_after_attempt(3))\n",
        "def chat_completion_request(messages, functions=None, function_call=None, model=\"gpt-3.5-turbo-0613\",temperature=0,max_tokens=None):\n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"Authorization\": \"Bearer \" + openai.api_key,\n",
        "    }\n",
        "    json_data = {\"model\": model, \"messages\": messages, \"temperature\": temperature,\"max_tokens\": max_tokens}\n",
        "\n",
        "    if functions is not None:\n",
        "        json_data.update({\"functions\": functions})\n",
        "    if function_call is not None:\n",
        "        json_data.update({\"function_call\": function_call})\n",
        "    try:\n",
        "        response = requests.post(\n",
        "            \"https://api.openai.com/v1/chat/completions\",\n",
        "            headers=headers,\n",
        "            json=json_data,\n",
        "        )\n",
        "        return response\n",
        "    except Exception as e:\n",
        "        print(\"Unable to generate ChatCompletion response\")\n",
        "        print(f\"Exception: {e}\")\n",
        "        return e"
      ],
      "metadata": {
        "id": "qwv5cRZGsB0i"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "biomed_decision_tree = {\n",
        "    'questions': ['What is your primary symptom?'],\n",
        "    'answers': {\n",
        "        'fever': {\n",
        "            'questions': ['Is your fever above 100.4F?'],\n",
        "            'answers': {\n",
        "                'yes': {\n",
        "                    'questions': ['Do you have other symptoms like cough or difficulty breathing?'],\n",
        "                    'answers': {\n",
        "                        'yes': 'Possible COVID-19. Please get tested immediately.',\n",
        "                        'no': 'Possible flu. Rest, hydrate, and consult with a healthcare professional if symptoms persist.'\n",
        "                    }\n",
        "                },\n",
        "                'no': {\n",
        "                    'questions': ['Do you have a rash?'],\n",
        "                    'answers': {\n",
        "                        'yes': 'Possibly a mild allergic reaction. Monitor symptoms and contact healthcare professional if condition worsens.',\n",
        "                        'no': 'It might be a mild infection. Rest and hydrate, but consult with a healthcare professional if symptoms persist.'\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "        },\n",
        "        'cough': {\n",
        "            'questions': ['Is your cough dry or productive?'],\n",
        "            'answers': {\n",
        "                'dry': {\n",
        "                    'questions': ['Do you have a fever or difficulty breathing?'],\n",
        "                    'answers': {\n",
        "                        'yes': 'Possible COVID-19. Please get tested immediately.',\n",
        "                        'no': 'Possibly a cold or allergies. Rest and hydrate, and consult a healthcare professional if symptoms persist.'\n",
        "                    }\n",
        "                },\n",
        "                'productive': {\n",
        "                    'questions': ['Are you experiencing shortness of breath?'],\n",
        "                    'answers': {\n",
        "                        'yes': 'Possibly pneumonia. Please consult with a healthcare professional immediately.',\n",
        "                        'no': 'Possibly bronchitis. Rest, hydrate, and consult a healthcare professional if symptoms persist.'\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "        },\n",
        "        'headache': {\n",
        "            'questions': ['Is the headache severe?', 'Do you have nausea or vomiting?'],\n",
        "            'answers': {\n",
        "                'yes': 'Possibly migraine or more serious conditions. Please consult with a healthcare professional immediately.',\n",
        "                'no': 'Possibly tension headache. Rest, hydrate, and consult a healthcare professional if symptoms persist.'\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "id": "So7ian2jwt2C"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BiomedQATree:\n",
        "    def __init__(self, decision_tree):\n",
        "        self.__tree = decision_tree\n",
        "        self.__current_node = self.__tree\n",
        "        self.__current_question_index = 0\n",
        "        self.__current_question = self.__get_current_question()\n",
        "        self.__possible_answers = self.__get_current_possible_answers()\n",
        "\n",
        "    def __get_current_question(self):\n",
        "        return self.__current_node['questions'][self.__current_question_index]\n",
        "\n",
        "    def __get_current_possible_answers(self):\n",
        "        return list(self.__current_node['answers'].keys())\n",
        "\n",
        "    def get_follow_up_q(self, answer):\n",
        "        if answer in self.__current_node['answers']:\n",
        "            next_node = self.__current_node['answers'][answer]\n",
        "\n",
        "            if isinstance(next_node, str):\n",
        "                advice = next_node\n",
        "                self.__current_node = self.__tree\n",
        "                self.__current_question_index = 0\n",
        "                self.__current_question = self.__get_current_question()\n",
        "                self.__possible_answers = self.__get_current_possible_answers()\n",
        "                return advice\n",
        "            else:\n",
        "                self.__current_node = next_node\n",
        "                self.__current_question_index = 0\n",
        "                self.__current_question = self.__get_current_question()\n",
        "                self.__possible_answers = self.__get_current_possible_answers()\n",
        "                return self.__current_question\n",
        "        else:\n",
        "            raise ValueError(\"Invalid answer, try again.\")\n",
        "\n",
        "        # Increment the current question index after returning the current question\n",
        "        if self.__current_question_index < len(self.__current_node['questions']) - 1:\n",
        "            self.__current_question_index += 1\n",
        "        else:\n",
        "            self.__current_question_index = 0\n",
        "\n",
        "    def get_current_question(self):\n",
        "        return self.__current_question\n",
        "\n",
        "    def get_current_possible_answers(self):\n",
        "        return self.__possible_answers\n"
      ],
      "metadata": {
        "id": "xnEgFmbbwyKj"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_schema(biomed_qa_tree):\n",
        "    current_question = biomed_qa_tree.get_current_question()\n",
        "    current_possible_answers = biomed_qa_tree.get_current_possible_answers()\n",
        "\n",
        "    schema_template = {\n",
        "        \"name\": \"get_follow_up_q\",\n",
        "        \"description\": f\"Given the current question '{current_question}', we try that the user gives us the answer. If we can consider the current question responded and retrieve the follow up question or final advice. Do not call this function if the user hasn't responded to the current question yet. Ask again or different is necessary but remember the current question {current_question}\",\n",
        "        \"call\": f\"biomed_qa_tree.get_follow_up_question({{answer: 'user answer'}})\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"answer\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"enum\": current_possible_answers,\n",
        "                    \"description\": \"The answer to the current question.\"\n",
        "                }\n",
        "            },\n",
        "            \"required\": [\"answer\"]\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return schema_template\n"
      ],
      "metadata": {
        "id": "skeukYSuzpH1"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chat():\n",
        "    tree= BiomedQATree(biomed_decision_tree)\n",
        "    messages = [None, {\"role\": \"assistant\", \"content\": \"Hello, how can I help you today?\"}]\n",
        "    while True:\n",
        "        functions = [generate_schema(tree)]\n",
        "        question = tree.get_current_question()\n",
        "        s = f\"\"\"You have only one job. Get the user to respond the current question '{question}'.\n",
        "                You are a question asker. Specifically the question '{question}'.\n",
        "                Be empathic and kind with the user and repeat the question in case it get's out of context.\n",
        "                Be really brief and remember your task. You have to get the answer to the current question.\n",
        "                Insist on different ways to get the user to answer.\"\"\"\n",
        "        messages[0] = {\"role\": \"system\", \"content\": s}\n",
        "\n",
        "\n",
        "        user_message = input()\n",
        "\n",
        "        if user_message == 'exit':\n",
        "            return 'Messages: ', messages\n",
        "\n",
        "        ## Append USER message:\n",
        "        messages.append({\"role\": \"user\", \"content\": user_message})\n",
        "\n",
        "        # Make API call\n",
        "        response = chat_completion_request(messages, functions=functions)#,function_call={\"name\":\"get_follow_up_q\"})\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # Handle response\n",
        "        if response.status_code == 200:\n",
        "            response_message = response.json()[\"choices\"][0][\"message\"]\n",
        "            ## Append response_message. Could be function_call or have further message content:\n",
        "            messages.append(response_message)\n",
        "            assistant_message = response_message['content']\n",
        "            current_question_answered = response_message.get(\"function_call\")\n",
        "            if current_question_answered:\n",
        "                answer = json.loads(current_question_answered['arguments'])['answer']\n",
        "                # If last message was a function call, append function response.\n",
        "                messages.append(\n",
        "                    {\n",
        "                        \"role\": \"function\",\n",
        "                        \"name\": \"get_follow_up_question\",\n",
        "                        \"content\": tree.get_follow_up_q(answer),\n",
        "                    }\n",
        "                )  # extend conversation with function response\n",
        "                print(tree.get_current_question(), tree.get_current_possible_answers())\n",
        "            # If we are in the function calling scenario, we need a second response with !=None content message.\n",
        "                second_response = chat_completion_request(messages, functions)\n",
        "                second_response_message = second_response.json()[\"choices\"][0][\"message\"]\n",
        "                messages.append(second_response_message)\n",
        "            print('------\\n',messages[-1])\n",
        "\n"
      ],
      "metadata": {
        "id": "3YCUbiKa06Bu"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 630
        },
        "id": "p7HMSBe978cP",
        "outputId": "e6a47674-b751-4254-a2cb-89b47169c6e4"
      },
      "execution_count": 108,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------\n",
            " {'role': 'assistant', 'content': \"I'm sorry to hear that you have a headache. It can be quite uncomfortable. To better understand your situation, may I ask what other symptoms you are experiencing?\"}\n",
            "------\n",
            " {'role': 'assistant', 'content': 'I understand. Thank you for letting me know about your headache. To assist you further, could you please tell me if there are any other symptoms you are experiencing?'}\n",
            "------\n",
            " {'role': 'assistant', 'content': 'Thank you for providing that information. I appreciate your patience. To better understand your situation, could you please let me know if there are any other symptoms you are experiencing besides the headache?'}\n",
            "------\n",
            " {'role': 'assistant', 'content': 'I apologize for the confusion. Thank you for letting me know that you are only experiencing a headache. Is there anything else you would like to share about your headache?'}\n",
            "Is the headache severe? ['yes', 'no']\n",
            "------\n",
            " {'role': 'assistant', 'content': 'Thank you for letting me know that you have a headache. I apologize for asking the same question again. To better understand your situation, could you please let me know if the headache is severe or mild?'}\n",
            "------\n",
            " {'role': 'assistant', 'content': \"I'm sorry to hear that you're experiencing such intense pain. It sounds like your headache is severe. Thank you for sharing that information. Is there anything else you would like to add about your headache?\"}\n",
            "What is your primary symptom? ['fever', 'cough', 'headache']\n",
            "------\n",
            " {'role': 'assistant', 'content': \"Thank you for letting me know that your headache is severe. I'm sorry to hear that you're experiencing such intense pain. It's important to take your symptoms seriously, especially if they are severe. I would recommend consulting with a healthcare professional to get a proper diagnosis and appropriate treatment. They will be able to provide you with the best advice and guidance. Is there anything else I can assist you with?\"}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-108-e57ee30c65ef>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mchat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-107-690866f66df4>\u001b[0m in \u001b[0;36mchat\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0muser_message\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muser_message\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'exit'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBd5kSaKXRds",
        "outputId": "6846054a-7daf-4bb9-cdcf-b66aeb2fccb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'role': 'system',\n",
              "  'content': \"You have only one job. Get the user to respond the current question .\\n        Be empathic and kind with the user and repeat the question in case it get's out of context.\\n        Be really brief and remember your task. You have to get the answer to the current question.\"},\n",
              " {'role': 'assistant', 'content': 'Hello, how can I help you today?'},\n",
              " []]"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivM5VY-wrkrp",
        "outputId": "8e75e5c0-6cb6-4cf0-9b18-8461bcef9b28"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'index': 0,\n",
              " 'message': {'role': 'assistant',\n",
              "  'content': 'Entiendo que tengas tus propias opiniones y creencias políticas, pero como asistente virtual, mi función principal es brindar información y ayudar con consultas generales. Si tienes alguna pregunta específica o necesitas ayuda con algo en particular, estaré encantado de ayudarte.'},\n",
              " 'finish_reason': 'stop'}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# question = \"Have you had a high fever recently? (high being more than 38 celsius, or more than 37.5 with discomfort)\"\n",
        "# s = f\"\"\"Whenever the user reports having a headache, your job is to ask the question '{question}'. In any other case, be kind and empathetic with the user but never get too out of context. You are a single task brief-talking NPC that acts cool until user reports having a headache and then you ask the question.\"\"\"\n",
        "# messages = []\n",
        "# messages.append({\"role\": \"system\", \"content\": s})\n",
        "# messages.append({\"role\": \"assistant\", \"content\": \"Hello, how can I help you today?\"})\n",
        "\n",
        "# def get_follow_up_question(text,messages, question,max_tokens):\n",
        "#     response = chat_completion_request(messages+[{\"role\": \"user\", \"content\":text}],max_tokens=max_tokens)\n",
        "#     return response.json()['choices'][0]\n",
        "\n",
        "# get_follow_up_question(\"vos crees en el peronismo o no?\", messages, question, max_tokens= int(len(question)))"
      ]
    }
  ]
}
